# A minimal viable Metaflow-on-Nebius stack

## What does it do?

It provisions all necessary Nebius AI Cloud resources. The main resources are:

* Nebius Object Storage
* Nebius Managed Service for Kubernetes

It will also deploy Metaflow services onto the Mk8s cluster above.

## Prerequisites

* Install [terraform](https://learn.hashicorp.com/tutorials/terraform/install-cli).
* Install [kubectl](https://kubernetes.io/docs/tasks/tools/#kubectl).
* Install [nebius](https://docs.nebius.com/cli/quickstart/) CLI and set it up.

## Usage

The templates are organized into two modules, `infra` and `services`.

Before you do anything, create a TF vars file `FILE.tfvars` (`FILE` could be something else), with this content.

```text
storage_container_name = "[YOUR USERNAME]-metaflow-storage-container" # A bucket name should be unique across the region.
```

This is used to help generate unique resource names for:

* Managed Service for PostgreSQL server name
* Nebius Object Storage storage account name

Note: these resources must be globally unique across all of Nebius.

Run `source ./.envrc.zsh` if you run bash, or `source ./.envrc.zsh` if you prefer zsh.

You can add tenant_id, project_id and vpc_subnet_id to `FILE.tfvars`

```text
tenant_id = "" # you can get from your console
project_id = "" # you can get from your console
vpc_subnet_id = "" # you can get from your console`
```

Next, apply the `infra` module (creates Nebius AI cloud resources only).

```bash
terraform init  
terraform apply -target="module.infra" -var-file=FILE.tfvars
```

Next, add the Service Account to the `editors` group and add values to `FILE.tfvars`. You can get values from the console from the Service Account page:

```txt
aws_access_key_id = ""
aws_secret_access_key=""
```

Then run:

```bash
terraform apply -target="module.services" -var-file=FILE.tfvars
```

## Destroying

To destroy infra run:

```bash
terraform destroy -target="module.infra" -var-file=FILE.tfvars
```

To destroy services run:

```bash
terraform destroy -target="module.services" -var-file=FILE.tfvars
```

### Airflow

**This is quickstart template only, not recommended for real production deployments**

If `deploy_airflow` is set to true, then the `services` module will deploy Airflow via a [helm chart](https://airflow.apache.org/docs/helm-chart/stable/index.html) into the kubernetes cluster (the one deployed by the `infra` module). 

The terraform template deploys Airflow configured with a `LocalExecutor`. Metaflow can work with any Airflow executor. This template deploys the `LocalExecutor` for simplicity.

After you have changed the value of `deploy_airflow`, reapply terraform for both [infra and services](#usage).

#### Shipping Metaflow compiled DAGs to Airflow
Airflow expects Python files with Airflow DAGS present in the [dags_folder](https://airflow.apache.org/docs/apache-airflow/2.2.0/configurations-ref.html#dags-folder). By default this terraform template uses the [defaults](https://airflow.apache.org/docs/helm-chart/stable/parameters-ref.html#airflow) set in the Airflow helm chart which is `{AIRFLOW_HOME}/dags` (`/opt/airflow/dags`).

The metaflow-tools repository also ships a [airflow_dag_upload.py](../../scripts/airflow_dag_upload.py) file that can help sync Airflow dag file generated by Metaflow to the Airflow scheduler _deployed by this template_. Under the hood [airflow_dag_upload.py](../../scripts/airflow_dag_upload.py) uses the `kubectl cp` command to copy files from local to the Airflow scheduler's container. Example of how to use the file:
```
python airflow_dag_upload.py my-dag.py /opt/airflow/dags/my-dag.py
```

## (Advanced) Terraform state management

Terraform manages the state of Nebius resources in [tfstate](https://www.terraform.io/language/state) files locally by default.

If you plan to maintain the minimal stack for any significant period of time, it is highly
recommended that these state files be stored in cloud storage (e.g. Nebius Object Storage) instead.

Some reasons include:

* More than one person needs to administer the stack (using terraform). Everyone should work off
  a single copy of tfstate.
* You wish to mitigate the risk of data-loss on your local disk.

For more details, see [Terraform docs](https://www.terraform.io/language/settings/backends/configuration).
