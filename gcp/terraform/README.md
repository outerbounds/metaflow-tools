# A minimal viable Metaflow-on-GCP stack

## What does it do?

It provisions all necessary GCP resources. Main resources are:

* Google Cloud Storage bucket
* GKE cluster
  It will also deploy Metaflow services onto the GKE cluster above.

## Prerequisites

* Install [terraform](https://learn.hashicorp.com/tutorials/terraform/install-cli).
* Install [kubectl](https://kubernetes.io/docs/tasks/tools/#kubectl).
* Install [gcloud](https://cloud.google.com/sdk/gcloud) CLI.
* `gcloud auth login` as a GCP account with sufficient privileges to administer necessary resources:
    * GKE
    * Google Cloud Storage bucket
    * Computer networks
    * IAM role assignments
    * ...
    * Note: If you are an owner of your GCP project already you should be good to go.

## Usage

The templates are organized into two modules, `infra` and `services`.

Before you do anything, create a `FILE.tfvars` file with the following content (updating relevant values):

```text
org_prefix = "<ORG_PREFIX>"
project = "<GCP_PROJECT_ID>"
db_generation_number = <DB_GENERATION_NUM>
```

For `org_prefix`, choose a short and memorable alphanumeric string. It will be used for naming the Google Cloud Storage bucket, whose
name must be globally unique across GCP.

For `GCP_PROJECT_ID`, set the GCP project ID you wish to use.

`DB_GENERATION_NUM` should be updated each time the DB instance is recreated. This helps generate unique DB instance
names.  Backstory: DB instance names cannot repeat within 7 day window on GCP.

You may rename `FILE.tfvars` to a more friendly name appropriate for your project.  E.g. `metaflow.poc.tfvars`.

The variable assignments defined in this file will be passed to `terraform` CLI.

Next, apply the `infra` module (creates GCP resources only).

```sh
terraform apply -target="module.infra" -var-file=FILE.tfvars
```

Then, apply the `services` module (deploys Metaflow services to GKE)

```sh
terraform apply -target="module.services" -var-file=FILE.tfvars
```

The step above will output next steps for Metaflow end users.

## Metaflow job orchestration options

The recommended way to orchestrate Metaflow workloads on Kubernetes is via [Argo Workflows](https://docs.metaflow.org/going-to-production-with-metaflow/scheduling-metaflow-flows/scheduling-with-argo-workflows). However, Airflow is also supported as an alternative.

The template also provides the `deploy_airflow` and `deploy_argo` flags as variables. These are booleans that specify if [Airflow](https://airflow.apache.org/) or [Argo Workflows](https://argoproj.github.io/argo-workflows/) will be deployed in the Kubernetes cluster along with Metaflow related services. By default `deploy_argo` is set to __true__ and `deploy_airflow` is set to __false__.
To change these, set them in your `FILE.tfvars` file (or else, via other [terraform variable](https://www.terraform.io/language/values/variables) passing mechanisms)

### Argo Workflows

Argo Workflows is installed by default on the AKS cluster as part of the `services` submodule. Setting the `deploy_argo` [variable](./variables.tf) will deploy Argo in the GKE cluster. No additional configuration is done in the `infra` module to support `argo`.

After you have changed the value of `deploy_argo`, re-apply terraform for both [infra and services](#usage).

### Airflow

> This is quickstart template only, not recommended for real production deployments

If `deploy_airflow`  is set to true, then the `services` module will deploy Airflow via a [helm chart](https://airflow.apache.org/docs/helm-chart/stable/index.html) into the kubernetes cluster (the one deployed by the `infra` module).

The terraform template deploys Airflow configured with a `LocalExecutor`. Metaflow can work with any Airflow executor. This template deploys the `LocalExecutor` for simplicity.

After you have changed the value of `deploy_airflow`, reapply terraform for both [infra and services](#usage).

#### Shipping Metaflow compiled DAGs to Airflow

Airflow expects Python files with Airflow DAGS present in the [dags_folder](https://airflow.apache.org/docs/apache-airflow/2.2.0/configurations-ref.html#dags-folder). By default this terraform template uses the [defaults](https://airflow.apache.org/docs/helm-chart/stable/parameters-ref.html#airflow) set in the Airflow helm chart which is `{AIRFLOW_HOME}/dags` (`/opt/airflow/dags`).

The metaflow-tools repository also ships a [airflow_dag_upload.py](../../scripts/airflow_dag_upload.py) file that can help sync Airflow dag file generated by Metaflow to the Airflow scheduler _deployed by this template_. Under the hood [airflow_dag_upload.py](../../scripts/airflow_dag_upload.py) uses the `kubectl cp` command to copy files from local to the Airflow scheduler's container. Example of how to use the file:

```sh
python airflow_dag_upload.py my-dag.py /opt/airflow/dags/my-dag.py
```

## (Advanced) Terraform state management

Terraform manages the state of GCP resources in [tfstate](https://www.terraform.io/language/state) files locally by default.

If you plan to maintain the minimal stack for any significant period of time, it is highly
recommended that these state files be stored in cloud storage (e.g. Google Cloud Storage) instead.

Some reasons include:

* More than one person needs to administer the stack (using terraform). Everyone should work off
  a single copy of tfstate.
* You wish to mitigate the risk of data-loss on your local disk.

For more details, see [Terraform docs](https://www.terraform.io/language/settings/backends/configuration).

> Currently, Terraform Cloud _will not work_ with these modules. Terraform Cloud doesn't have `kubectl` available and cannot create Argo or Airflow services correctly.
